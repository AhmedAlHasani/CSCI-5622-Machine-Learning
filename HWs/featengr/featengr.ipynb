{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $Ahmed Al Hasani$ \n",
    "\n",
    "**Kaggle Username**: $<mandazi11>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score\n",
    "from csv import DictReader, DictWriter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "from nltk import ngrams\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming\n",
    "class TDIDF_Stemming():\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def stem_tokens(self, tokens, stemmer):\n",
    "        stemmed = []\n",
    "        \n",
    "        for item in tokens:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        \n",
    "        return stemmed\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        tokens = word_tokenize(examples)\n",
    "        tokens = [i for i in tokens if i not in string.punctuation and i != \"michael\" ]\n",
    "        stemmed = self.stem_tokens(tokens, self.stemmer)\n",
    "        return stemmed\n",
    "\n",
    "#Count the length of a sentence \n",
    "class sentence_length_transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, examples):\n",
    "        return self\n",
    "\n",
    "    def transform(self, examples):\n",
    "        X = np.zeros((len(examples), 1))\n",
    "        \n",
    "        for ii, x in enumerate(examples):\n",
    "            X[ii,:] = np.array([len(x)])\n",
    "\n",
    "        return csr_matrix(X)\n",
    "\n",
    "#Part of Speech Tokenizer\n",
    "class POS_tokenizer(object):\n",
    "    def __call__(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        words_and_pos_tags = pos_tag(words)\n",
    "        return [word_and_pos[0] for word_and_pos in words_and_pos_tags if word_and_pos[1] != \"NN\" \\\n",
    "        and word_and_pos[1] != \"IN\" and word_and_pos[1] != \"PRP$\"]\n",
    "\n",
    "def find_word_spoiler(data, word, information):\n",
    "        text = data [information]\n",
    "        tags = data [\"spoiler\"]\n",
    "        count_true = 0 \n",
    "        count_false = 0\n",
    "        count = 0\n",
    "\n",
    "        key_words = [word]\n",
    "        print(key_words)\n",
    "        for sentence, tag in zip(text, tags):\n",
    "            if any(word in sentence.lower() for word in key_words):\n",
    "                if str(tag)=='True':\n",
    "                    count_true+=1\n",
    "                elif str(tag)=='False':\n",
    "                    count_false+=1\n",
    "                count+=1\n",
    "\n",
    "        print(\"Total Sentences: \" + str(count))\n",
    "        print(\"Total Spoilers: \" + str(count_true))\n",
    "        print(\"Total Non-Spoilers: \" + str(count_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TD IDF and CountVectorizer\n",
    "binary = false, which means a binary text model is not set, hence, bag-of-words will be used\n",
    "min_df = minimum document frequency. If terms did not occur in many documents, ignore that term. \n",
    "max_df = opposite to above\n",
    "strip_accents = ascii / unicode / none\n",
    "sublinear_tf = sublear tf scaling, replace tf with 1+log(tf)\n",
    "ngrams = unigram, bigrams or ngrams. I used bigrams\n",
    "norm = to normalize, default is l2\n",
    "'''\n",
    "\n",
    "'''\n",
    "Function Transformer\n",
    "validate: checks the array X beforehand.  True or False\n",
    "accept_sparse: True or False\n",
    "'''\n",
    "\n",
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        self.Y = ['True', 'False']\n",
    "        self.vectorizer = FeatureUnion([(\n",
    "            \"sentence_tfidfVect\", Pipeline([('sentece', FunctionTransformer(lambda x:x[0], validate = False)),\n",
    "                ('tfid', TfidfVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english'))])),\n",
    "            (\"trope_countVect\", Pipeline([('trope', FunctionTransformer(lambda x:x[1], validate = False)), \n",
    "                ('countvectorizer', CountVectorizer())]))\n",
    "            ])\n",
    "        \n",
    "        self.data = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "        \n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\" \n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        #write data in dictionary, convert to list\n",
    "        dfTrain = list(DictReader(open(\"../data/spoilers/train.csv\")))\n",
    "\n",
    "        #grab different information from dictionary above\n",
    "        self.X_train = self.build_train_features([[x[\"sentence\"] for x in dfTrain], [x[\"trope\"] for x in dfTrain]])\n",
    "        \n",
    "        #grab spoilers, convert them to 0's and 1's\n",
    "        self.y_train = np.array(list(['True', 'False'].index(x[\"spoiler\"]) for x in dfTrain))\n",
    "        \n",
    "        k_folds_test = KFold(n_splits=10, shuffle=True)\n",
    "        accuracy = []\n",
    "        for train_index, test_index in k_folds_test.split(self.X_train):\n",
    "            local_x_train, local_x_test = self.X_train[train_index], self.X_train[test_index]\n",
    "            local_y_train, local_y_test = self.y_train[train_index], self.y_train[test_index]\n",
    "\n",
    "            self.logreg = LogisticRegression(random_state=1230)\n",
    "            self.logreg.fit(local_x_train, local_y_train)\n",
    "            local_y_pred = self.logreg.predict(local_x_test)\n",
    "            accurate = accuracy_score(local_y_test, local_y_pred)\n",
    "\n",
    "            accuracy.append(accurate)\n",
    "            print('Local Accuracy: ', accurate)\n",
    "        \n",
    "        print('Avg Accuracy is: ', sum(accuracy) / len(accuracy))\n",
    "\n",
    "        #train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv =10)\n",
    "        print(scores)\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        # read in test data \n",
    "        dfTest = list(DictReader(open(\"../data/spoilers/test.csv\")))\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features([[x[\"sentence\"] for x in dfTest], [x[\"trope\"] for x in dfTest]])\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "\n",
    "        #increment id as each line is written\n",
    "        id_csv = 0\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        with open(\"prediction.csv\", \"w\") as output:\n",
    "            wr = DictWriter(output, fieldnames=[\"Id\", \"spoiler\"], lineterminator = '\\n')\n",
    "            wr.writeheader()\n",
    "\n",
    "            for p in pred:\n",
    "                d = {\"Id\": id_csv, \"spoiler\": self.Y[p]}\n",
    "                wr.writerow(d)\n",
    "                id_csv+=1\n",
    "        \n",
    "    def computeLength(self):\n",
    "        count_true = 0.0\n",
    "        count_false = 0.0\n",
    "        total_true_length = 0.0\n",
    "        total_false_length = 0.0\n",
    "        for index, row  in self.data.iterrows():\n",
    "            if row[\"spoiler\"] == True:\n",
    "                count_true += 1.0\n",
    "                total_true_length += len(row[\"sentence\"])\n",
    "            else:\n",
    "                count_false += 1.0\n",
    "                total_false_length += len(row[\"sentence\"])\n",
    "\n",
    "        print(\"Avg Length For Spoilers: \", end=\"\")\n",
    "        print(total_true_length/count_true)\n",
    "        print(\"Avg Length For Non-Spoilers: \", end=\"\")\n",
    "        print(total_false_length/count_false)\n",
    "        print(\"Total No. of Spoilers: \" + str(count_true))\n",
    "        print(\"Total No. of Non-Spoilers: \" + str(count_false))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Accuracy:  0.769423558897\n",
      "Local Accuracy:  0.74269005848\n",
      "Local Accuracy:  0.738512949039\n",
      "Local Accuracy:  0.750208855472\n",
      "Local Accuracy:  0.770258980785\n",
      "Local Accuracy:  0.74269005848\n",
      "Local Accuracy:  0.751879699248\n",
      "Local Accuracy:  0.758563074353\n",
      "Local Accuracy:  0.763575605681\n",
      "Local Accuracy:  0.747702589808\n",
      "Avg Accuracy is:  0.753550543024\n",
      "[ 0.67111853  0.6903172   0.66583124  0.67919799  0.60818713  0.6566416\n",
      "  0.62406015  0.62907268  0.63963211  0.64966555]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1230)\n",
    "\n",
    "# Shows the top 10 features for each class \n",
    "#feat.show_top10()\n",
    "\n",
    "# Make prediction on test data and produce Kaggle submission file \n",
    "feat.model_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  My Approach\n",
    "Initially, the features names extracted using only countvectorizer for my transformer were: \n",
    "Pos: tear freya dies harvey sebastian regina morgana olivia moriarty destiny\n",
    "Neg: cory johnny tim drew often hilarious meant cody disney fed\n",
    "\n",
    "Even before submitting the prediction csv file from the above features, I was able to conclude that the features would not be helpful in determining whether a sentence has a spoiler, because they focused heavily on names rather than verbs or other words that indicate a plot twist, and hence, vital information that concludes a sentence as a spoiler. \n",
    "\n",
    "After submitting the file, it resulted in the baseline score of ~0.62231.\n",
    "\n",
    "Here I wanted to test several approaches.\n",
    "First: Key words \n",
    "Key words that are often found in spoilers vary, and I guessed they were similar to words \"kill\" , \"spoiler\", \"dead\", and \"end\".\n",
    "As a result, the cell below shows a simple for loop that counts how many True and False for each word. \n",
    "\n",
    "#key_words = [\"spoiler\", \"spoilers\"] #7 vs 1\n",
    "#key_words = [\"death\"] #208 vs 55\n",
    "#key_words = [\"died\"] #92 vs 33\n",
    "#key_words = [\"killed\"] #178 vs 39\n",
    "#key_words = [\"kills\"] #77 vs 19\n",
    "#key_words = ['stabs'] #7 vs 2\n",
    "#key_words = ['ending'] #97 vs 37\n",
    "#key_words = ['revenge'] #37 vs 9\n",
    "#key_words = ['suicide'] #49 vs 10\n",
    "#key_words = ['chances'] #9 vs 1\n",
    "#key_words = ['tear'] #23 vs 3\n",
    "#key_words = ['spoiler', 'spoilers', 'death', 'killed', 'kills', 'stabs', 'suicide', 'revenge'] #462 vs 115\n",
    "\n",
    "This verified my assumption that verbs and other words that are related to a change in situation are highly related to spoilers. An example is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spoiler']\n",
      "Total Sentences: 8\n",
      "Total True: 7\n",
      "Total False: 1\n"
     ]
    }
   ],
   "source": [
    "word = 'spoiler'\n",
    "information = 'sentence'\n",
    "find_word_spoiler(feat.data, word, information )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, I want a vectorizer that will give me top features with similar words. I tried setting self.vectorizer to a Tf Idf vectorizer as (One of the vectorizers Chris suggested in class) to test what are the top features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos: end die dies kills dead olivia revealed death killed finale\n",
      "Neg: show often usually tim drew always started cast like than\n"
     ]
    }
   ],
   "source": [
    "class FeatEngr_TFIDF:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(use_idf = False)\n",
    "    def build_train_features(self, examples):\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "    def get_test_features(self, examples):\n",
    "        return self.vectorizer.transform(examples)\n",
    "    def show_top10(self):\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "    def train_model(self, random_state=1234):\n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        self.X_train = self.build_train_features(list(dfTrain[\"sentence\"]))\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "feat_tfidf = FeatEngr_TFIDF()\n",
    "feat_tfidf.train_model(random_state=1230)\n",
    "feat_tfidf.show_top10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top features from the TF IDF vectorizer, all words seem to fit what is expected in a spoiler, in addition to the small test of finding how many spoilers vs non spoilers for each word I conducted. \n",
    "I submitted the predictions from the test above, and it resulted in a Kaggle Score of ~0.67.\n",
    "\n",
    "Clearly, every word in the top 10 features seem to fit, except for olivia. It is undesirable to have a name in the highest weights to classify a sentence as a spoiler. A name might indicate a spoiler only for a certain show, which is not helpful, because the program must be able to detect spoilers for various shows. This must be fixed.\n",
    "Additionally, using tokenizer and stemming might provide me with an improved transformer. Rather than have die or dies in the highest words as seperate terms to look for, using stem() will help in getting rid of 'dies' and hence, I will have another term there that fits better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I read this paper (http://www.umiacs.umd.edu/~jbg/docs/2013_spoiler.pdf) on their approach in detecting spoilers through feature engineering. They noted that the longer a post, the more likely it is to contain as spoiler. This is due two several reasons. The first is, a longer post means more descriptions and more information to describe a situation, it is likely this post is a spoiler. Second, longer posts describe lengthy shows. This means friendlier shows, such as sitcoms, are unlikely talked about in that post, and instead, shows that take a long time to develop a plot and the story are more likely discussed in that particular post. Hence, it is also important to consider the genre of the show or the length of the post.\n",
    "\n",
    "I wanted to find the length of posts that are spoilers and nonspoilers to determine whether it is a worth approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Length For Spoilers: 120.85846055979644\n",
      "Avg Length For Non-Spoilers: 104.15047518479409\n",
      "Total No. of Spoilers: 6288.0\n",
      "Total No. of Non-Spoilers: 5682.0\n"
     ]
    }
   ],
   "source": [
    "feat.computeLength()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, I believed a custom transformer to count the length of posts is not worth it, because length of spoilers are slightly longer than non-spoiler posts. However, when I counted the number of spoilers, a noticeably larger amount of spoilers did not lower the average length of spoilers to a point lower than the average length for non-spoilers. \n",
    "\n",
    "I created a vectorizer using Feature Union that combined TF IDF with stemming and counting the length of the posts from the class I created just as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself.vectorizer = FeatureUnion([(\"TfidfVectorizer\", TfidfVectorizer(ngram_range=(1, 2), stop_words=\\'english\\', tokenizer=TDIDF_Stemming())), \\n        (\"sentence_length_transformer\", sentence_length_transformer())])\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "self.vectorizer = FeatureUnion([(\"TfidfVectorizer\", TfidfVectorizer(ngram_range=(1, 2), stop_words='english', tokenizer=TDIDF_Stemming())), \n",
    "        (\"sentence_length_transformer\", sentence_length_transformer())])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that time, I did not use cross validation to test my vectorizer, and submitted the predictions from using the transformer from the resultant self.vectorizer. I only earned a score of ~0.63, and this continued for several submissions for similar vectorizers as above, each time adjusting the parameters to TF IDF while also using the sentence_length_transformer. Scores varied between 0.60 and 0.64, and only when I had limited submissions and time did I start using cross validation and considered different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatEngr3:\n",
    "    def __init__(self):\n",
    "        \n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        self.vectorizer = FeatureUnion([(\"TDIDF:\" , TfidfVectorizer(min_df = 110, use_idf = False, tokenizer = TDIDF_Stemming(), stop_words='english')), \n",
    "           (\"Count:\", CountVectorizer(ngram_range=(1, 2), stop_words='english', tokenizer=POS_tokenizer()))])\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "                \n",
    "    def train_model(self, random_state=1234):\n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        self.X_train = self.build_train_features(list(dfTrain[\"sentence\"]))\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "\n",
    "        k_folds_test = KFold(n_splits=10, shuffle=True)\n",
    "        accuracy = []\n",
    "        for train_index, test_index in k_folds_test.split(self.X_train):\n",
    "            local_x_train, local_x_test = self.X_train[train_index], self.X_train[test_index]\n",
    "            local_y_train, local_y_test = self.y_train[train_index], self.y_train[test_index]\n",
    "\n",
    "            self.logreg = LogisticRegression(random_state=1230)\n",
    "            self.logreg.fit(local_x_train, local_y_train)\n",
    "            local_y_pred = self.logreg.predict(local_x_test)\n",
    "            accurate = accuracy_score(local_y_test, local_y_pred)\n",
    "\n",
    "            accuracy.append(accurate)\n",
    "            print('Local Accuracy: ', accurate)\n",
    "        \n",
    "        print('Avg Accuracy is: ', sum(accuracy) / len(accuracy))\n",
    "\n",
    "        #train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv =10)\n",
    "        print(scores)\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(list(dfTest[\"sentence\"]))\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Accuracy:  0.644110275689\n",
      "Local Accuracy:  0.644110275689\n",
      "Local Accuracy:  0.637426900585\n",
      "Local Accuracy:  0.624060150376\n",
      "Local Accuracy:  0.644945697577\n",
      "Local Accuracy:  0.637426900585\n",
      "Local Accuracy:  0.634920634921\n",
      "Local Accuracy:  0.654135338346\n",
      "Local Accuracy:  0.623224728488\n",
      "Local Accuracy:  0.621553884712\n",
      "Avg Accuracy is:  0.636591478697\n",
      "[ 0.62186978  0.61769616  0.62823726  0.61403509  0.58228906  0.61236424\n",
      "  0.61236424  0.59314954  0.590301    0.63628763]\n",
      "Pos: Count:__sebastian Count:__starting Count:__kills Count:__turns TDIDF:__kill TDIDF:__die TDIDF:__reveal TDIDF:__death TDIDF:__end TDIDF:__final\n",
      "Neg: Count:__small Count:__actually , Count:__hilarious Count:__meant TDIDF:__live Count:__frequently TDIDF:__like Count:__problems Count:__drew Count:__# 2\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feat3 = FeatEngr3()\n",
    "\n",
    "feat3.train_model(random_state=1230)\n",
    "\n",
    "feat3.show_top10()\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now I exhausted tokens, stemming, CountVectorizer and Tf-Idf with their parameters without progress. Parameters included playing around with bigrams and ngrams, minimum document frequency to catch words that occur frequently and others. It is important to note thought that using bigrams, stop words, and lower case was the most helpful. Additionally, combining countvectorizer and tdidf did not result in meaningful features, in fact, countvectorizer was not a meaningful additional to the results I found from using tf idf alone. Also, Using sentence length did not help either. The accurracy scores and the cross validation scores were also low. \n",
    "\n",
    "I watched a YouTube Video that discussed general approaches to extract information, which included Stemming and bigrams (https://www.youtube.com/watch?v=oYe03Y1WQaI&t=328s)\n",
    "\n",
    "Below is an example of using Tf Idf with stop words, bigrams, and lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatEngr_TFIDF2:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True, stop_words='english')\n",
    "    def build_train_features(self, examples):\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "    def get_test_features(self, examples):\n",
    "        return self.vectorizer.transform(examples)\n",
    "    def show_top10(self):\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "    def train_model(self, random_state=1234):\n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        self.X_train = self.build_train_features(list(dfTrain[\"sentence\"]))\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        k_folds_test = KFold(n_splits=10, shuffle=True)\n",
    "        accuracy = []\n",
    "        for train_index, test_index in k_folds_test.split(self.X_train):\n",
    "            local_x_train, local_x_test = self.X_train[train_index], self.X_train[test_index]\n",
    "            local_y_train, local_y_test = self.y_train[train_index], self.y_train[test_index]\n",
    "\n",
    "            self.logreg = LogisticRegression(random_state=1230)\n",
    "            self.logreg.fit(local_x_train, local_y_train)\n",
    "            local_y_pred = self.logreg.predict(local_x_test)\n",
    "            accurate = accuracy_score(local_y_test, local_y_pred)\n",
    "\n",
    "            accuracy.append(accurate)\n",
    "            print('Local Accuracy: ', accurate)\n",
    "        \n",
    "        print('Avg Accuracy is: ', sum(accuracy) / len(accuracy))\n",
    "\n",
    "        #train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv =10)\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Accuracy:  0.664160401003\n",
      "Local Accuracy:  0.66081871345\n",
      "Local Accuracy:  0.680033416876\n",
      "Local Accuracy:  0.697577276525\n",
      "Local Accuracy:  0.680033416876\n",
      "Local Accuracy:  0.68253968254\n",
      "Local Accuracy:  0.702589807853\n",
      "Local Accuracy:  0.674185463659\n",
      "Local Accuracy:  0.695906432749\n",
      "Local Accuracy:  0.69089390142\n",
      "Avg Accuracy is:  0.682873851295\n",
      "[ 0.63105175  0.64607679  0.64578112  0.63324979  0.62155388  0.62489557\n",
      "  0.60568087  0.54636591  0.57943144  0.66220736]\n",
      "Pos: kill kills dies dead end revealed finale death turns killed\n",
      "Neg: usually like tim drew cast tv cory seasons meant ryan\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feat_tfidf2 = FeatEngr_TFIDF2()\n",
    "feat_tfidf2.train_model(random_state=1230)\n",
    "feat_tfidf2.show_top10()\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that I verified that using these parameters resulted in more accurate features, and it also got rid of the names, such as Olivia. However, this does not solve the meaningless addition of CountVectorizer. \n",
    "\n",
    "I considered then using countvectorizer on tropes alone to see if I can find a correlation to the theme of the sentence rather then the length alone. In the cell below, I will create a class FeatEngr using only countvectorizor and instead of self.X_train grabbing information from the sentence column, I will change it to the trope column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatEngr4:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        return self.vectorizer.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-10:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:10]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "                \n",
    "    def train_model(self, random_state=1234):\n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        self.X_train = self.build_train_features(list(dfTrain[\"trope\"]))\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "\n",
    "        k_folds_test = KFold(n_splits=10, shuffle=True)\n",
    "        accuracy = []\n",
    "        for train_index, test_index in k_folds_test.split(self.X_train):\n",
    "            local_x_train, local_x_test = self.X_train[train_index], self.X_train[test_index]\n",
    "            local_y_train, local_y_test = self.y_train[train_index], self.y_train[test_index]\n",
    "\n",
    "            self.logreg = LogisticRegression(random_state=1230)\n",
    "            self.logreg.fit(local_x_train, local_y_train)\n",
    "            local_y_pred = self.logreg.predict(local_x_test)\n",
    "            accurate = accuracy_score(local_y_test, local_y_pred)\n",
    "\n",
    "            accuracy.append(accurate)\n",
    "            print('Local Accuracy: ', accurate)\n",
    "        \n",
    "        print('Avg Accuracy is: ', sum(accuracy) / len(accuracy))\n",
    "\n",
    "        #train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv =10)\n",
    "        print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Accuracy:  0.727652464495\n",
      "Local Accuracy:  0.736006683375\n",
      "Local Accuracy:  0.739348370927\n",
      "Local Accuracy:  0.727652464495\n",
      "Local Accuracy:  0.749373433584\n",
      "Local Accuracy:  0.723475355054\n",
      "Local Accuracy:  0.736006683375\n",
      "Local Accuracy:  0.744360902256\n",
      "Local Accuracy:  0.739348370927\n",
      "Local Accuracy:  0.732664995823\n",
      "Avg Accuracy is:  0.735588972431\n",
      "[ 0.63439065  0.67278798  0.61487051  0.6449457   0.60651629  0.61236424\n",
      "  0.6031746   0.6374269   0.65384615  0.61789298]\n",
      "Pos: neverfoundthebody foreshadowing killedoffforreal heroicbsod backfromthedead ohcrap bittersweetending xanatosgambit thereveal whamepisode\n",
      "Neg: catchphrase deadpansnarker thecastshowoff abc flanderization spinoff gameshow stockfootage thebbc domcom\n",
      "------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feat4 = FeatEngr4()\n",
    "\n",
    "feat4.train_model(random_state=1230)\n",
    "\n",
    "feat4.show_top10()\n",
    "print(\"------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is when I made my biggest breakthrough. I looked at the top 10 tropes for the positive class (spoilers) and they describe situations that are more likely to be spoilers. This was a suitable alternative to sentence length. Instead of extracting sentence length to guess what content it is, why not use the trope directly? Additionally, my accuracy is significantly higher than previous tests.\n",
    "\n",
    "Additionally, my local accuracy values were the highest here, although the cross validation scores were lower compared to using TF IDF alone. Nonetheless, using TF IDF on sentences and CountVectorizer on tropes seemed more promising. \n",
    "\n",
    "Further, I wanted to see if the suggested trope above are related to spoilers and not just a fluke or mistake by CountVectorizer. So I ran a few tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neverfoundthebody']\n",
      "Total Sentences: 14\n",
      "Total Spoilers: 14\n",
      "Total Non-Spoilers: 0\n"
     ]
    }
   ],
   "source": [
    "word = 'neverfoundthebody'\n",
    "information = 'trope'\n",
    "find_word_spoiler(feat.data, word, information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foreshadowing']\n",
      "Total Sentences: 66\n",
      "Total Spoilers: 58\n",
      "Total Non-Spoilers: 8\n"
     ]
    }
   ],
   "source": [
    "word = 'foreshadowing'\n",
    "information = 'trope'\n",
    "find_word_spoiler(feat.data, word, information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the negative class? I also wanted to test the features suggested by CountVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['catchphrase']\n",
      "Total Sentences: 50\n",
      "Total Spoilers: 3\n",
      "Total Non-Spoilers: 47\n"
     ]
    }
   ],
   "source": [
    "word = 'catchphrase'\n",
    "information = 'trope'\n",
    "find_word_spoiler(feat.data, word, information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deadpansnarker']\n",
      "Total Sentences: 24\n",
      "Total Spoilers: 1\n",
      "Total Non-Spoilers: 23\n"
     ]
    }
   ],
   "source": [
    "word = 'deadpansnarker'\n",
    "information = 'trope'\n",
    "find_word_spoiler(feat.data, word, information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further tests with different tropes this seemed like a viable option for my transformer. Additionally, using tropes is promising because they explain a general idea or theme that gather sentences under a specific \"umbrella\", so sentences under the trope neverfoundthebody will definitely hold key information that classify a sentence as a spoiler. \n",
    "\n",
    "I wanted to feed my Tf Idf vectorizer with sentences only, and feed my CountVectorizer with tropes only. As a result, these 2 websites provided a helpful guide to only selecting certain information for building a customized transformer. They suggested using FunctionTransformer. \n",
    "1) https://stackoverflow.com/questions/43274423/use-sklearns-functiontransformer-with-string-data\n",
    "2) http://scikit-learn.org/stable/auto_examples/preprocessing/plot_function_transformer.html\n",
    "\n",
    "Additionally, using Pipeline will gather only one FunctionTransformer for Tf Idf and another one for Countvectorizer, then all I need to do is gather the two Pipelines with Feature Union. Although the get_feature_names function will not work on the pipeline, I realized with the previous tests that it will result in tropes and key words that accurately classify a sentence as a spoiler, so all I am doing now is feeding the arguments to those vectors using Function Transformer via Pipeline. And even though I can't call get_feature_names, the high accuracy scores validated my assumption that I had better features from combining the two vectors. It was my final adjustment and it resulted in the highest accuracy and scores from the cross validation test.\n",
    "\n",
    "I changed the dfTrain variable given to us initially so that I can create dictionary with information from the sentence and trope column only, then convert it to a list. I did this for the training part and the predicting part. The resultant accuracy and cross validation scores were:\n",
    "\n",
    "Local Accuracy:  0.743525480368\n",
    "Local Accuracy:  0.756056808688\n",
    "Local Accuracy:  0.774436090226\n",
    "Local Accuracy:  0.770258980785\n",
    "Local Accuracy:  0.741019214703\n",
    "Local Accuracy:  0.738512949039\n",
    "Local Accuracy:  0.739348370927\n",
    "Local Accuracy:  0.741854636591\n",
    "Local Accuracy:  0.758563074353\n",
    "Local Accuracy:  0.730158730159\n",
    "Avg Accuracy is:  0.749373433584\n",
    "[ 0.67111853  0.6903172   0.66583124  0.67919799  0.60818713  0.6566416\n",
    "  0.62406015  0.62907268  0.63963211  0.64966555]\n",
    "  \n",
    "The local accuracy was significantly higher then all previous tests, including the cross validation scores. With three submissions left on Thursday night, I felt confident in submitting the resultant CSV file and it resulted in the score of ~0.71. The final program is the one I posted in Part 1 in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
